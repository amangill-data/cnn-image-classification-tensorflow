{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4503e7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.15.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6787840a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (50000, 32, 32, 3) Labels: (50000,)\n",
      "Test set shape: (10000, 32, 32, 3) Labels: (10000,)\n"
     ]
    }
   ],
   "source": [
    "cifar_dir = os.path.expanduser(\"~/JPTR_NTBK/CIFAR_10_DATA/cifar-10-batches-py/\")\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        data_dict = pickle.load(fo, encoding='bytes')\n",
    "    return data_dict\n",
    "\n",
    "def load_cifar_data():\n",
    "    X_train, y_train = [], []\n",
    "\n",
    "    for i in range(1, 6):  \n",
    "        data_batch = unpickle(os.path.join(cifar_dir, f\"data_batch_{i}\"))\n",
    "        X_train.append(data_batch[b'data'])\n",
    "        y_train.append(data_batch[b'labels'])\n",
    "\n",
    "    X_train = np.concatenate(X_train, axis=0).reshape(-1, 32, 32, 3)\n",
    "    y_train = np.concatenate(y_train, axis=0)\n",
    "\n",
    "    \n",
    "    test_batch = unpickle(os.path.join(cifar_dir, \"test_batch\"))\n",
    "    X_test = test_batch[b'data'].reshape(-1, 32, 32, 3)\n",
    "    y_test = np.array(test_batch[b'labels'])\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_cifar_data()\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape, \"Labels:\", y_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape, \"Labels:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a46ee4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e805da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()  \n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 32, 32, 3], name=\"X\")\n",
    "y = tf.placeholder(tf.int32, [None], name=\"y\")\n",
    "\n",
    "conv1 = tf.layers.conv2d(X, filters=32, kernel_size=3, padding='same', activation=tf.nn.relu)\n",
    "pool1 = tf.layers.max_pooling2d(conv1, pool_size=2, strides=2)\n",
    "\n",
    "conv2 = tf.layers.conv2d(pool1, filters=64, kernel_size=3, padding='same', activation=tf.nn.relu)\n",
    "pool2 = tf.layers.max_pooling2d(conv2, pool_size=2, strides=2)\n",
    "\n",
    "flat = tf.layers.flatten(pool2)\n",
    "\n",
    "fc1 = tf.layers.dense(flat, units=128, activation=tf.nn.relu)\n",
    "\n",
    "logits = tf.layers.dense(fc1, units=10)\n",
    "\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.cast(y, tf.int64))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0960a199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)  \n",
    "    print(\"Model initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05dd6f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 7\n",
      "Epoch 0: Loss = 2.3379, Test Accuracy = 10.0000%\n",
      "Epoch 100: Loss = 1.8263, Test Accuracy = 33.5300%\n",
      "Epoch 200: Loss = 1.5205, Test Accuracy = 42.0500%\n",
      "Epoch 300: Loss = 1.4317, Test Accuracy = 45.8200%\n",
      "Epoch 400: Loss = 1.3529, Test Accuracy = 45.7300%\n",
      "Epoch 500: Loss = 1.1013, Test Accuracy = 49.2300%\n",
      "Epoch 600: Loss = 1.6794, Test Accuracy = 50.1200%\n",
      "Epoch 700: Loss = 1.5378, Test Accuracy = 51.1200%\n",
      "Epoch 800: Loss = 1.1003, Test Accuracy = 51.4300%\n",
      "Epoch 900: Loss = 1.2741, Test Accuracy = 55.4600%\n",
      "Epoch 1000: Loss = 1.4220, Test Accuracy = 54.2600%\n",
      "Epoch 1100: Loss = 1.3912, Test Accuracy = 55.0200%\n",
      "Epoch 1200: Loss = 1.1747, Test Accuracy = 56.7000%\n",
      "Epoch 1300: Loss = 1.1948, Test Accuracy = 57.1500%\n",
      "Epoch 1400: Loss = 1.0090, Test Accuracy = 57.5800%\n",
      "Epoch 1500: Loss = 1.1698, Test Accuracy = 56.6500%\n",
      "Epoch 1600: Loss = 1.1001, Test Accuracy = 58.2400%\n",
      "Epoch 1700: Loss = 0.9210, Test Accuracy = 58.9300%\n",
      "Epoch 1800: Loss = 0.9249, Test Accuracy = 58.2700%\n",
      "Epoch 1900: Loss = 0.9541, Test Accuracy = 60.0800%\n",
      "Epoch 2000: Loss = 1.0699, Test Accuracy = 60.2200%\n",
      "Epoch 2100: Loss = 1.1283, Test Accuracy = 61.0200%\n",
      "Epoch 2200: Loss = 1.0090, Test Accuracy = 59.7600%\n",
      "Epoch 2300: Loss = 1.1830, Test Accuracy = 60.5600%\n",
      "Epoch 2400: Loss = 1.0958, Test Accuracy = 61.1900%\n",
      "Epoch 2500: Loss = 1.0258, Test Accuracy = 62.3300%\n",
      "Epoch 2600: Loss = 1.2241, Test Accuracy = 60.9600%\n",
      "Epoch 2700: Loss = 1.0606, Test Accuracy = 61.1300%\n",
      "Epoch 2800: Loss = 0.9187, Test Accuracy = 61.7100%\n",
      "Epoch 2900: Loss = 0.9584, Test Accuracy = 62.1000%\n",
      "Epoch 3000: Loss = 0.8954, Test Accuracy = 62.3500%\n",
      "Epoch 3100: Loss = 0.9016, Test Accuracy = 62.4000%\n",
      "Epoch 3200: Loss = 1.0428, Test Accuracy = 62.5500%\n",
      "Epoch 3300: Loss = 0.9887, Test Accuracy = 63.5600%\n",
      "Epoch 3400: Loss = 1.0619, Test Accuracy = 62.9800%\n",
      "Epoch 3500: Loss = 1.0676, Test Accuracy = 63.6800%\n",
      "Epoch 3600: Loss = 0.8467, Test Accuracy = 63.5000%\n",
      "Epoch 3700: Loss = 0.7602, Test Accuracy = 63.3600%\n",
      "Epoch 3800: Loss = 0.7807, Test Accuracy = 62.7000%\n",
      "Epoch 3900: Loss = 1.0903, Test Accuracy = 63.6100%\n",
      "Epoch 4000: Loss = 0.8429, Test Accuracy = 63.8200%\n",
      "Epoch 4100: Loss = 0.8628, Test Accuracy = 62.2000%\n",
      "Epoch 4200: Loss = 0.6988, Test Accuracy = 63.1600%\n",
      "Epoch 4300: Loss = 0.9518, Test Accuracy = 63.1500%\n",
      "Epoch 4400: Loss = 0.9255, Test Accuracy = 63.8300%\n",
      "Epoch 4500: Loss = 0.9076, Test Accuracy = 64.4500%\n",
      "Epoch 4600: Loss = 0.9876, Test Accuracy = 63.4500%\n",
      "Epoch 4700: Loss = 0.7424, Test Accuracy = 64.5800%\n",
      "Epoch 4800: Loss = 0.5582, Test Accuracy = 63.1000%\n",
      "Epoch 4900: Loss = 0.8359, Test Accuracy = 63.2700%\n",
      "Training complete!\n",
      "Final Test Accuracy: 63.1400%\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5000  \n",
    "batch_size = 64     \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)  \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        indices = np.random.choice(len(X_train), batch_size)\n",
    "        X_batch, y_batch = X_train[indices], y_train[indices]\n",
    "\n",
    "        _, loss_value = sess.run([optimizer, loss], feed_dict={X: X_batch, y: y_batch}\n",
    "        if epoch % 100 == 0:\n",
    "            acc = sess.run(accuracy, feed_dict={X: X_test, y: y_test})\n",
    "            print(f\"Epoch {epoch}: Loss = {loss_value:.4f}, Test Accuracy = {acc:.4%}\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "    final_acc = sess.run(accuracy, feed_dict={X: X_test, y: y_test})\n",
    "    print(f\"Final Test Accuracy: {final_acc:.4%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efcf5c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
